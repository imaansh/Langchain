{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODWSQnXfCHRplSaOcPi1Mo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imaansh/Langchain/blob/main/Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6hgw4CGO4O9Y",
        "outputId": "10f62a07-36a8-4a2e-8faf-1167a610c141"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.4/404.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.6/55.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m602.6/602.6 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m646.7/646.7 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m228.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-groq langchain langchain_core \"langchain-chroma>=0.1.2\" langchain_community sentence_transformers fastembed"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_EYp8ks45rf",
        "outputId": "9bc7e309-1fc4-4671-e089-a652de2de6fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_groq import ChatGroq\n",
        "\n",
        "model = ChatGroq (\n",
        "    model = \"llama3-groq-8b-8192-tool-use-preview\",\n",
        "    temperature = 0,\n",
        "    max_tokens = None,\n",
        "    timeout = None,\n",
        "    max_retries = 2\n",
        ")"
      ],
      "metadata": {
        "id": "6G5TAnWB52el"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage"
      ],
      "metadata": {
        "id": "huZlzJxI7wXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "\n",
        "            SystemMessage(content=\"Solve this math problem\"),\n",
        "            HumanMessage(content=\"What is 81 divided by 9\")\n",
        "]\n",
        "\n",
        "result = model.invoke(messages)\n",
        "print(result)\n",
        "print(\"Contents Only:\")\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63Bknoo_8cA6",
        "outputId": "281718e0-356f-4a43-eabd-473be51dcf7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='81 divided by 9 is 9.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 28, 'total_tokens': 38, 'completion_time': 0.008379798, 'prompt_time': 0.005306847, 'queue_time': 0.007467243, 'total_time': 0.013686645}, 'model_name': 'llama3-groq-8b-8192-tool-use-preview', 'system_fingerprint': 'fp_260dc69250', 'finish_reason': 'stop', 'logprobs': None} id='run-bbd1be7f-9efb-4cb4-97ce-8fc2d99fdfe6-0' usage_metadata={'input_tokens': 28, 'output_tokens': 10, 'total_tokens': 38}\n",
            "Contents Only:\n",
            "81 divided by 9 is 9.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"Solve this math problem\"),\n",
        "    HumanMessage(content=\"What is 81 divided by 9\"),\n",
        "    AIMessage(content=\"81 divided by 9 is 9\"),\n",
        "    HumanMessage(content=\"What is the square root of 81\")\n",
        "\n",
        "]\n",
        "result = model.invoke(messages)\n",
        "print(result)\n",
        "print(\"Contents Only:\")\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hi4zAhj9taa",
        "outputId": "e4a39b2e-28a9-4e1d-aec6-2d09857de0d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='The square root of 81 is 9' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 54, 'total_tokens': 64, 'completion_time': 0.008376648, 'prompt_time': 0.008022856, 'queue_time': 0.006716784, 'total_time': 0.016399504}, 'model_name': 'llama3-groq-8b-8192-tool-use-preview', 'system_fingerprint': 'fp_260dc69250', 'finish_reason': 'stop', 'logprobs': None} id='run-0fe252e7-702a-491c-979f-6983200894cd-0' usage_metadata={'input_tokens': 54, 'output_tokens': 10, 'total_tokens': 64}\n",
            "Contents Only:\n",
            "The square root of 81 is 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "messages =[\n",
        "\n",
        "           (\"system: You are a comedian who tells jokes about {topic}\"),\n",
        "           HumanMessage(content=\"Tell me {count} jokes\")\n",
        "\n",
        "]\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
        "prompt = prompt_template.invoke({\"topic\":\"cats\",\"count\":3})\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWbpykqhDP6-",
        "outputId": "621dc1c2-30ba-480a-b5db-0470fa497b8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "messages=[HumanMessage(content='system: You are a comedian who tells jokes about cats', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me {count} jokes', additional_kwargs={}, response_metadata={})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template_multiple = \"\"\"You are a helpful assistant.\n",
        "Human: Tell me a {adjective} short story about a {animal}.\n",
        "Assistant:\"\"\"\n",
        "prompt_multiple = ChatPromptTemplate.from_template(template_multiple)\n",
        "prompt = prompt_multiple.invoke({\"adjective\": \"funny\", \"animal\": \"panda\"})\n",
        "\n",
        "result = model.invoke(prompt)\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAkdh9KEFqIr",
        "outputId": "fbd0e18a-366f-4548-b780-cb53b46abbc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One day, a panda named Ping decided to learn how to surf. He found a surf school on the beach and signed up for lessons. The instructor was surprised to see a panda in a wetsuit, but Ping was determined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.output_parser import StrOutputParser\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "    (\"system\", \"You are a comedian who tells jokes about {topic}\"),\n",
        "    (\"human\", \"Tell me {count} jokes\")\n",
        "    ]\n",
        "\n",
        ")\n",
        "\n",
        "chain = prompt_template | model | StrOutputParser()\n",
        "\n",
        "result = chain.invoke({\"topic\":\"cats\",\"count\":3})\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7o_17kbVOiQs",
        "outputId": "0a73ad4f-d7aa-4c43-8cdd-2e13053b55a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are three jokes about cats:\n",
            "\n",
            "1. Why did the cat join a band? Because it wanted to be the purr-cussionist!\n",
            "2. Why did the cat go to the vet? To get its paws-itive diagnosis!\n",
            "3. What did the cat say when it was happy? \"I'm feline great!\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are an expert product reviewer\"),\n",
        "        (\"human\", \"List the main features of the product {product_name}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "def analyze_pros(features):\n",
        "    pros_template = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"You are an expert product reviewer\"),\n",
        "            (\"human\", \"Given the features: {features}, list the pros of these features\")\n",
        "        ]\n",
        "    )\n",
        "    return pros_template.format_prompt(features=features)\n",
        "\n",
        "\n",
        "def analyze_cons(features):\n",
        "    cons_template = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", \"You are an expert product reviewer\"),\n",
        "            (\"human\", \"Given the features: {features}, list the cons of these features\")\n",
        "        ]\n",
        "    )\n",
        "    return cons_template.format_prompt(features=features)\n",
        "\n",
        "\n",
        "def combine_pros_cons(pros, cons):\n",
        "    return f\"Pros: {pros}\\nCons: {cons}\"\n",
        "\n",
        "\n",
        "pros_branch_chain = (\n",
        "    RunnableLambda(lambda x: analyze_pros(x)) | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "cons_branch_chain = (\n",
        "    RunnableLambda(lambda x: analyze_cons(x)) | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "chain = (\n",
        "    prompt_template\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        "    | RunnableParallel({\"pros\": pros_branch_chain, \"cons\": cons_branch_chain})\n",
        "    | RunnableLambda(lambda x: combine_pros_cons(x[\"pros\"], x[\"cons\"]))\n",
        ")\n",
        "\n",
        "result = chain.invoke({\"product_name\": \"iPhone\"})\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvvwhoPlPjpi",
        "outputId": "e1493158-308b-4846-953a-0cb3fc36e090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pros: Based on the features of the iPhone, here are some of the pros:\n",
            "\n",
            "1. **Multi-Touch Screen**: The multi-touch screen allows for intuitive and interactive user experience.\n",
            "2. **User-Friendly Operating System**: iOS is designed to be easy to use and navigate.\n",
            "3. **High-Quality Camera**: The camera offers excellent photo and video quality, with additional features like optical zoom and portrait mode.\n",
            "4. **Access to App Store**: The App Store provides a wide range of apps for various needs.\n",
            "5. **Wireless Connectivity**: Wi-Fi, Bluetooth, and cellular connectivity ensure constant access to the internet and other devices.\n",
            "6. **Long Battery Life**: iPhones generally have a long battery life, making them suitable for daily use.\n",
            "7. **Sleek and Durable Design**: The design is both aesthetically pleasing and durable.\n",
            "8. **Strong Security Features**: Face ID or Touch ID provide robust security for user data.\n",
            "9. **Water Resistance**: Many models are water-resistant, making them suitable for use in wet conditions.\n",
            "10. **Seamless Hardware and Software Integration**: The integration of hardware and software provides a smooth user experience.\n",
            "Cons: It's challenging to find significant cons for the features listed, as they are generally well-regarded. However, here are a few potential drawbacks:\n",
            "\n",
            "1. **Display**: Some users might find the screen too small or too large for their preferences.\n",
            "2. **Operating System**: iOS can be restrictive for users who prefer more customization options.\n",
            "3. **Camera**: While the camera quality is generally excellent, some users might find it lacking in certain situations, such as low light.\n",
            "4. **App Store**: The App Store's strict guidelines can limit the availability of certain apps.\n",
            "5. **Wireless Connectivity**: Users might experience occasional connectivity issues or slow speeds in certain areas.\n",
            "6. **Battery Life**: Despite being generally good, battery life can vary significantly depending on usage patterns, and some users might find it insufficient.\n",
            "7. **Design**: The design might not appeal to everyone's taste, and some users might find it too fragile or bulky.\n",
            "8. **Security**: While security features are strong, they can sometimes interfere with functionality or require frequent updates.\n",
            "9. **Water Resistance**: Water resistance can be limited, and users should still exercise caution around water.\n",
            "10. **Hardware and Software Integration**: While seamless integration is a strength, it can sometimes make it difficult to troubleshoot issues or upgrade individual components.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnableBranch\n",
        "\n",
        "\n",
        "positive_feedback_template = ChatPromptTemplate.from_messages(\n",
        "\n",
        "    {\n",
        "        (\"system\", \"You are a helpful assistant\"),\n",
        "        (\"human\",\"Generate a thank you note for this positive feedback: {feedback}\")\n",
        "    }\n",
        ")\n",
        "\n",
        "negative_feedback_template = ChatPromptTemplate.from_messages(\n",
        "\n",
        "    {\n",
        "        (\"system\", \"You are a helpful assistant\"),\n",
        "        (\"human\",\"Generate a response addressing this negative feedback: {feedback}\")\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "neutral_feedback_template = ChatPromptTemplate.from_messages(\n",
        "\n",
        "    {\n",
        "        (\"system\", \"You are a helpful assistant\"),\n",
        "        (\"human\",\"Generate a request for more details for this neutral feedback: {feedback}\")\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "escalate_feedback_template = ChatPromptTemplate.from_messages(\n",
        "\n",
        "    {\n",
        "        (\"system\", \"You are a helpful assistant\"),\n",
        "        (\"human\",\"Generate a message to escalate this feedback to a human agent: {feedback}\")\n",
        "    }\n",
        ")\n",
        "\n",
        "classification_template = ChatPromptTemplate.from_messages(\n",
        "\n",
        "    {\n",
        "        (\"system\", \"You are a helpful assistant\"),\n",
        "        (\"human\",\"Classify the sentiment of this feedback as positive, negative, neutral, or escalate: {feedback}\")\n",
        "    }\n",
        ")\n",
        "\n",
        "\n",
        "branches = RunnableBranch (\n",
        "    (\n",
        "        lambda x: \"positive\" in x,\n",
        "        positive_feedback_template | model | StrOutputParser()\n",
        "    ),\n",
        "\n",
        "    (\n",
        "        lambda x: \"negative\" in x,\n",
        "        negative_feedback_template | model | StrOutputParser()\n",
        "    ),\n",
        "\n",
        "    (\n",
        "     lambda x: \"neutral\" in x,\n",
        "     neutral_feedback_template | model | StrOutputParser ()\n",
        "\n",
        "    ),\n",
        "    escalate_feedback_template | model | StrOutputParser()\n",
        ")\n",
        "\n",
        "\n",
        "classification_chain = classification_template | model | StrOutputParser()\n",
        "\n",
        "chain = classification_chain | branches\n",
        "\n",
        "review = \"The product is too bad.\"\n",
        "result = chain.invoke({\"feedback\": review})\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKQk5JVFTL-p",
        "outputId": "81185688-614e-460a-8796-85a172da3ecb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm sorry to hear that you had a negative experience. Could you please provide more details about what went wrong? This will help me understand the issue better and assist you more effectively.\n"
          ]
        }
      ]
    }
  ]
}